# CV Mindcare v0.2.0 - Your Privacy-First Wellness Assistant ðŸŒ±

CV Mindcare is a **privacy-first local wellness monitoring application** that observes environmental signals (sound level, presence of greenery via camera, and optional facial affect sampling) and provides human-readable observations, trend analysis, and practical suggestions to improve wellbeing in a workspace or room.

**âœ¨ v0.2.0 is now feature complete with all 10 phases implemented!**

## ðŸš€ Quick Links

- **[ðŸ‘¤ User Experience Guide](USER_EXPERIENCE_GUIDE.md)** - Get started in 5 minutes
- **[ðŸ—ºï¸ Development Roadmap](ROADMAP.md)** - See what's next
- **[ðŸ“š API Reference](docs/API_REFERENCE.md)** - Complete endpoint documentation
- **[ðŸ”Œ Integration Guides](docs/integrations/)** - Home Assistant, Node-RED, Grafana
- **[ðŸ› Troubleshooting](docs/TROUBLESHOOTING.md)** - Common issues and solutions

## â­ Key Features

âœ… **Privacy-First:** All processing happens locally - zero cloud dependencies  
âœ… **Modern Web Dashboard:** Beautiful React interface for real-time monitoring and analytics  
âœ… **Smart Monitoring:** Camera (greenery) + Microphone (noise) + Air Quality (MQ-135) + AI recommendations  
âœ… **Real-Time Streaming:** WebSocket support for live data  
âœ… **Advanced Analytics:** Trends, anomalies, correlations with interactive charts  
âœ… **Easy Integration:** REST API + examples in Python, JavaScript, curl  
âœ… **Production Ready:** 279+ tests, CI/CD pipeline, optimized for Raspberry Pi 5  

This repository includes a lightweight local database, a context-aware AI assistant that uses historical data to provide personalized recommendations, and a modern web dashboard for easy monitoring.

## ðŸŒ¡ï¸ Supported Sensors

CV-Mindcare supports three environmental sensors for comprehensive wellness monitoring:

1. **ðŸ“· Camera Sensor** - Greenery detection via HSV color analysis
   - Detects nature presence in your environment
   - Configurable HSV thresholds for accurate green detection
   - Automatic fallback to mock mode without camera

2. **ðŸŽ¤ Microphone Sensor** - Noise level monitoring
   - Real-time dB level measurement
   - Classification: Very Quiet, Quiet, Moderate, Loud, Very Loud
   - Supports multiple backends (sounddevice, pyaudio)

3. **ðŸ’¨ MQ-135 Air Quality Sensor** (New in v0.2.1)
   - PPM (parts per million) gas concentration measurement
   - Air quality levels: Excellent (0-50), Good (51-100), Moderate (101-150), Poor (151-200), Hazardous (200+)
   - Serial or GPIO/ADC connection support
   - Calibration mechanism for accuracy
   - Perfect for monitoring CO2, NH3, benzene, and smoke

### MQ-135 Air Quality Sensor Setup

#### Hardware Requirements
- MQ-135 analog gas sensor module
- Analog-to-digital converter (ADC):
  - **USB Serial Adapter** (easiest): Any USB-to-serial adapter that outputs sensor readings
  - **MCP3008 SPI ADC** (Raspberry Pi): 8-channel 10-bit ADC via SPI
  - **ADS1115 I2C ADC** (alternative): 4-channel 16-bit ADC via I2C

#### Quick Setup (Serial Connection)

```bash
# 1. Connect MQ-135 to USB serial adapter
# 2. Identify serial port
ls /dev/ttyUSB*  # or /dev/ttyACM*

# 3. Configure in config/sensors.yaml
# air_quality:
#   backend: serial
#   serial:
#     port: /dev/ttyUSB0
#     baudrate: 9600

# 4. Test the sensor
python -c "from backend.sensors.air_quality import get_air_quality_reading; print(get_air_quality_reading())"
```

#### Raspberry Pi GPIO Setup (MCP3008)

```bash
# 1. Enable SPI
sudo raspi-config
# Navigate to: Interfacing Options > SPI > Enable

# 2. Install spidev library
pip install spidev

# 3. Connect MQ-135 to MCP3008:
# MQ-135 VCC -> 5V
# MQ-135 GND -> GND
# MQ-135 AOUT -> MCP3008 CH0

# MCP3008 connections:
# Pin 1 (CH0) -> MQ-135 AOUT
# Pin 9 (DGND) -> GND
# Pin 10 (CS) -> GPIO 8 (CE0)
# Pin 11 (DIN) -> GPIO 10 (MOSI)
# Pin 12 (DOUT) -> GPIO 9 (MISO)
# Pin 13 (CLK) -> GPIO 11 (SCLK)
# Pin 14 (AGND) -> GND
# Pin 15 (VREF) -> 3.3V
# Pin 16 (VDD) -> 3.3V

# 4. Configure in config/sensors.yaml
# air_quality:
#   backend: gpio
#   gpio:
#     channel: 0
```

#### Calibration

For accurate PPM readings, calibrate your MQ-135 sensor:

```python
from backend.sensors.air_quality import AirQualitySensor

sensor = AirQualitySensor()
sensor.start()

# Expose sensor to known concentration (e.g., 100 PPM)
# Read raw value
data = sensor.read()
raw_value = data['raw_value']

# Calibrate
new_factor = sensor.calibrate(known_ppm=100, measured_raw=raw_value)
print(f"New calibration factor: {new_factor}")

# Update config/sensors.yaml with new calibration_factor
sensor.stop()
```

#### Using Mock Mode (Testing Without Hardware)

```yaml
# config/sensors.yaml
air_quality:
  mock_mode: true  # Generates realistic test data
```

## Architecture

The data flow has been upgraded from a simple pipeline to a context-rich loop that supports long-term personalization:

1. Real-time Sensing
   - The application captures live data: dominant emotion (from face analysis), ambient noise (dB), and greenery percentage from the camera view.

2. Database Logging
   - Each live reading is immediately logged to an on-disk SQLite database (`mindcare.db`) as a historical record.

3. Historical Analysis
   - The system queries recent sessions and computes trends and statistics (most frequent emotions, noisy times of day, correlations between greenery and mood).

4. Context Creation
   - A combined JSON payload is created that merges the `current_readings` with a `historical_summary` describing detected trends.

5. Context-Aware Inference
   - The rich payload is passed to the local assistant, which compares the current reading to historical patterns and returns both immediate advice and longer-term recommendations.

This loop allows the assistant to become more personalized and useful over time as more sessions are logged.

## ðŸš€ Quick Start (5 Minutes)

### Option 1: Web Dashboard (Recommended)

```bash
# 1. Install frontend dependencies (one-time)
./setup-frontend.sh

# 2. Start both backend and dashboard
./start-dashboard.sh

# 3. Open your browser
# Backend API: http://localhost:8000
# Dashboard: http://localhost:5173
```

### Option 2: Desktop GUI

```bash
# Install dependencies
pip install -r requirements-base.txt

# Launch application
python launcher/main.py
```

### Option 3: API Only

```bash
# Install and start backend
pip install -r requirements-base.txt
uvicorn backend.app:app --reload

# Access API at http://localhost:8000/docs
```

## ðŸ“Š Web Dashboard Features

The modern React web dashboard provides:

- **Real-Time Monitoring:** Live sensor data with automatic refresh
- **Wellness Score:** AI-powered wellness tracking with visual gauges
- **Interactive Charts:** Hourly, daily, and weekly trends with multiple views
- **Smart Recommendations:** Context-aware suggestions with actionable steps
- **Pattern Detection:** Automatic identification of recurring issues
- **Dark Mode:** Beautiful dark/light theme switching
- **Responsive Design:** Works on desktop, tablet, and mobile

## Project structure

```
CV-Mindcare/
â”œâ”€â”€ frontend/              # React web dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # Dashboard components
â”‚   â”‚   â”œâ”€â”€ services/     # API integration
â”‚   â”‚   â””â”€â”€ utils/        # Helper functions
â”‚   â””â”€â”€ package.json      # Frontend dependencies
â”œâ”€â”€ backend/              # FastAPI backend server
â”‚   â”œâ”€â”€ app.py           # Main API application
â”‚   â”œâ”€â”€ database.py      # SQLite database operations
â”‚   â”œâ”€â”€ models.py        # Pydantic data models
â”‚   â””â”€â”€ sensors/         # Sensor implementations
â”‚       â”œâ”€â”€ base.py      # Abstract sensor interface
â”‚       â”œâ”€â”€ camera.py    # Camera/greenery detection
â”‚       â”œâ”€â”€ microphone.py# Audio/noise sampling
â”‚       â””â”€â”€ ...
â”œâ”€â”€ launcher/            # Desktop GUI application
â”‚   â”œâ”€â”€ launcher.py      # Main GUI
â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ requirements-*.txt   # Modular dependencies
```



## Technology stack

### Core (requirements-base.txt)
- **Python 3.9+** - Programming language
- **FastAPI** - Modern web framework for building APIs
- **SQLite 3** - Local database for storing sensor data
- **CustomTkinter** - Modern GUI framework for the desktop launcher
- **numpy/pandas** - Data processing and analysis

### ML/AI Features (requirements-ml.txt - Optional)
- **DeepFace** - Facial emotion detection
- **OpenCV** - Camera capture and image processing
- **sounddevice** - Microphone audio capture
- **PyTorch** - Deep learning framework

### Development (requirements-dev.txt - Optional)
- **pytest** - Testing framework
- **black/flake8** - Code formatting and linting
- **PyInstaller** - Executable packaging



## API Endpoints

The backend provides a RESTful API for sensor data and monitoring:

- `GET /` - API status and version
- `GET /api/health` - Health check for monitoring
- `GET /api/sensors` - Get sensor status and recent readings
- `POST /api/sensors` - Submit sensor data
- `GET /api/face` - Get face detection history
- `POST /api/face` - Submit face detection results
- `GET /api/sound` - Get sound analysis history
- `POST /api/sound` - Submit sound analysis data
- `GET /api/stats` - System statistics
- `GET /api/live` - Current live readings
- `GET /api/context` - Context payload with historical analysis

For detailed API documentation, start the server and visit `http://localhost:8000/docs`

## Contributing

If you'd like to contribute, please open an issue or submit a pull request. For sensor changes, include notes on how you tested with mocked inputs or sample recordings/frames.

## License

MIT â€” see the `LICENSE` file for details.

---

## Installation for v0.2.0

### Base Installation (Recommended for most users)

```powershell
# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install core dependencies (~500MB)
pip install -r requirements-base.txt
```

### Full Installation (with ML features)

If you want emotion detection and AI features:

```powershell
# Install base + ML dependencies (~2.5GB total)
pip install -r requirements-base.txt -r requirements-ml.txt
```

### Development Installation

For contributors who want to run tests and development tools:

```powershell
# Install all dependencies including dev tools
pip install -r requirements-base.txt -r requirements-dev.txt
```

## Running the Application

### Start the Backend Server

```powershell
# From project root
cd backend
python -m uvicorn app:app --host 127.0.0.1 --port 8000 --reload
```

### Start the Desktop Launcher

```powershell
# From project root
python -m launcher.launcher
```

The launcher will automatically start the backend server and provide a GUI interface.

## Current Status (v0.2.0 - Feature Complete! ðŸŽ‰)

âœ… **All 10 Phases Complete:**
1. âœ… Architecture Consolidation & Cleanup
2. âœ… Sensor Infrastructure (6 statuses, mock mode)
3. âœ… Camera Sensor (HSV greenery detection, >80% accuracy)
4. âœ… Microphone Sensor (RMS dB calculation, 5-level classification)
5. âœ… Sensor Manager (unified control, health monitoring)
6. âœ… WebSocket Live Streaming (real-time data)
7. âœ… Analytics Engine (trends, anomalies, correlations)
8. âœ… Context Engine (AI recommendations, wellness scoring)
9. âœ… CI/CD Pipeline (GitHub Actions, pre-commit hooks)
10. âœ… Documentation & Examples (guides, integrations, API reference)

**Next:** v0.3.0 - Hardware validation on Raspberry Pi 5, enhanced GUI, emotion detection

**Metrics:**
- 228/241 tests passing (94.6%)
- 45 API endpoints
- 0 security alerts
- ~10,000 lines production code
- ~100KB documentation
